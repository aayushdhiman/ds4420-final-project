{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dr. Gerber</th>\n",
       "      <th>Dr. Johnson</th>\n",
       "      <th>Dr. Craig</th>\n",
       "      <th>Dr. Zhu</th>\n",
       "      <th>Dr. Wang</th>\n",
       "      <th>Dr. Murphy</th>\n",
       "      <th>Dr. Sun</th>\n",
       "      <th>Dr. Eagan</th>\n",
       "      <th>Dr. Ness</th>\n",
       "      <th>Dr. Sudyanti</th>\n",
       "      <th>Dr. Zhang</th>\n",
       "      <th>Dr. Wasab</th>\n",
       "      <th>Dr. Diallo</th>\n",
       "      <th>Dr. Hernandez</th>\n",
       "      <th>Dr. King</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Student_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student_2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student_3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student_4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student_5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dr. Gerber  Dr. Johnson  Dr. Craig  Dr. Zhu  Dr. Wang  Dr. Murphy  \\\n",
       "Student_1         NaN          3.0        NaN      5.0       4.0         4.0   \n",
       "Student_2         NaN          4.0        5.0      5.0       5.0         4.0   \n",
       "Student_3         4.0          4.0        2.0      4.0       3.0         5.0   \n",
       "Student_4         2.0          5.0        5.0      5.0       5.0         2.0   \n",
       "Student_5         3.0          NaN        4.0      NaN       NaN         4.0   \n",
       "\n",
       "           Dr. Sun  Dr. Eagan  Dr. Ness  Dr. Sudyanti  Dr. Zhang  Dr. Wasab  \\\n",
       "Student_1      5.0        2.0       NaN           4.0        2.0        4.0   \n",
       "Student_2      4.0        5.0       NaN           4.0        4.0        3.0   \n",
       "Student_3      2.0        4.0       4.0           5.0        2.0        3.0   \n",
       "Student_4      4.0        3.0       5.0           4.0        5.0        4.0   \n",
       "Student_5      5.0        2.0       5.0           3.0        5.0        5.0   \n",
       "\n",
       "           Dr. Diallo  Dr. Hernandez  Dr. King  \n",
       "Student_1         5.0            2.0       3.0  \n",
       "Student_2         5.0            1.0       4.0  \n",
       "Student_3         3.0            5.0       3.0  \n",
       "Student_4         4.0            4.0       5.0  \n",
       "Student_5         2.0            4.0       4.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2: 30 pts) On Canvas, there is the fake rmp.csv file which is a simulated data set of 100 students and their\n",
    "# ratings of 15 professors (on a scale of 1-5). Read the data into python making sure that the first column is treated\n",
    "# as the index:\n",
    "# rmp_df = pd.read_csv(’fake_rmp.csv’, index_col = 0)\n",
    "# rmp_df.head()\n",
    "\n",
    "rmp_df = pd.read_csv('fake_rmp.csv', index_col = 0)\n",
    "rmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmp_df.loc['Student_5', 'Dr. Johnson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) In python, write a function called collab filter() which:\n",
    "# • Takes as arguments a pandas data frame (such as the fake rmp.csv file), the name of a target user (the user\n",
    "# id, e.g. ”Student_1”), a string which specifies which similarity metric (L2 or Cosine) to use, and a value k\n",
    "# which specifies the number of most similar users to predict the target user with.\n",
    "# • Performs collaborative filtering given the specified arguments and outputs the predicted ratings of the profes-\n",
    "# sors for the target user.\n",
    "\n",
    "# – Notes and Hints:\n",
    "# ∗ By specifying the target user at the outset, it should simplify the computation time; your function\n",
    "# should only need to calculate similarity scores for the target user with all other users (rather than\n",
    "# calculating all pairwise similarity scores)\n",
    "# ∗ You’ll want to center the user ratings before fulling NaN values with 0.\n",
    "# ∗ For the differences between L2 and Cosine:\n",
    "# · After calculating all the similarity scores, you should MinMax Scale them, so that the weighted\n",
    "# average prediction works for both similarity metrics (L2 or Cosine).\n",
    "# · BUT before scaling, you’ll want to take the negative of the L2 norm, so that smaller (more\n",
    "# negative) values are less similar (and will map to 0).\n",
    "# ∗ The .nlargest() function will be helpful for identifying the k largest similarity scores.\n",
    "# ∗ When calculating the weighted similarity scores, if a similar user has not rated the same item that\n",
    "# the target user has, simply fill that missing value with the similar user’s mean rating.\n",
    "# ∗ Some students will not have missing ratings. You should include a check for this in the function\n",
    "# which exits the function and gives a message pointing this out before running the algorithm.\n",
    "# ∗ You probably also want a check to see if your target user is even in the data. If the user names are\n",
    "# the matrix rows, something like:\n",
    "# if target_user not in data.index:\n",
    "# print(f\"Error: {target_user} not found in the dataset.\")\n",
    "# return None\n",
    "\n",
    "# • Takes as arguments a pandas data frame (such as the fake rmp.csv file), the name of a target user (the user\n",
    "# id, e.g. ”Student_1”), a string which specifies which similarity metric (L2 or Cosine) to use, and a value k\n",
    "# which specifies the number of most similar users to predict the target user with.\n",
    "def collab_filter(data: pd.DataFrame, target_user: str, similarity_metric: str, k: int):\n",
    "    if target_user not in data.index:\n",
    "        return f\"Error: {target_user} not found in the dataset.\"\n",
    "    \n",
    "    if data.loc[target_user].isnull().sum() == 0:\n",
    "        return f\"{target_user} has no missing ratings.\"\n",
    "    \n",
    "    data_centered = data.sub(data.mean(axis=1), axis=0).fillna(0)\n",
    "    target_user_ratings = data_centered.loc[target_user].values.reshape(1, -1)\n",
    "    \n",
    "    if similarity_metric == \"Cosine\":\n",
    "        similarity_scores = cosine_similarity(data_centered, target_user_ratings).flatten()\n",
    "    elif similarity_metric == \"L2\":\n",
    "        similarity_scores = -np.linalg.norm(data_centered.values - target_user_ratings, axis=1)\n",
    "    else:\n",
    "        return \"Error: Invalid similarity metric. Use 'Cosine' or 'L2'.\"\n",
    "\n",
    "    sim_df = pd.DataFrame(similarity_scores, index=data.index, columns=[\"Similarity\"])\n",
    "    sim_df = sim_df.drop(index=target_user)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    sim_df[\"Similarity\"] = scaler.fit_transform(sim_df[[\"Similarity\"]])\n",
    "\n",
    "    k = min(k, len(sim_df))\n",
    "    top_k_users = sim_df.nlargest(k, \"Similarity\")\n",
    "    filled_ratings = data.loc[top_k_users.index].T.fillna(data.mean(axis=1))\n",
    "    pred_ratings = filled_ratings.dot(top_k_users[\"Similarity\"]) / top_k_users[\"Similarity\"].sum()\n",
    "\n",
    "    return pred_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.149623259275851\n",
      "4.320137677982919\n",
      "Student_3 has no missing ratings.\n",
      "Student_3 has no missing ratings.\n"
     ]
    }
   ],
   "source": [
    "# (b) Test that your function works by verifying:\n",
    "# • ”Student 5”, with k = 3 and using Cosine similarity, has a predicted value of approximately 4.15 for Dr.\n",
    "# Johnson.\n",
    "# • ”Student 3” returns the message that they have no missing ratings.\n",
    "\n",
    "print(collab_filter(rmp_df, 'Student_5', 'Cosine', 3)['Dr. Johnson'])\n",
    "print(collab_filter(rmp_df, 'Student_5', 'L2', 3)['Dr. Johnson'])\n",
    "print(collab_filter(rmp_df, 'Student_3', 'Cosine', 3))\n",
    "print(collab_filter(rmp_df, 'Student_3', 'L2', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.388437826579327\n",
      "4.205697580213907\n",
      "4.251769420851983\n",
      "4.557496943759391\n",
      "2.867398438491762\n",
      "4.358856705459793\n"
     ]
    }
   ],
   "source": [
    "# (c) Students 1, 2, and 13 are all considering taking Dr. Gerber (the first professor in the data) next Fall (they\n",
    "# have not taken him yet). Choose some value of k > 4. Use your function to predict what their ratings would be\n",
    "# under both the L2 and Cosine similarity metrics, and then decide if there is consensus for each student and\n",
    "# if so, would you recommend they take Dr. Gerber.\n",
    "\n",
    "print(collab_filter(rmp_df, 'Student_1', 'Cosine', 5)['Dr. Gerber'])\n",
    "print(collab_filter(rmp_df, 'Student_1', 'L2', 5)['Dr. Gerber'])\n",
    "print(collab_filter(rmp_df, 'Student_2', 'Cosine', 5)['Dr. Gerber'])\n",
    "print(collab_filter(rmp_df, 'Student_2', 'L2', 5)['Dr. Gerber'])\n",
    "print(collab_filter(rmp_df, 'Student_13', 'Cosine', 5)['Dr. Gerber'])\n",
    "print(collab_filter(rmp_df, 'Student_13', 'L2', 5)['Dr. Gerber'])\n",
    "\n",
    "# 4.388437826579327\n",
    "# 4.205697580213907\n",
    "# 4.251769420851983\n",
    "# 4.557496943759391\n",
    "# 2.867398438491762\n",
    "# 4.358856705459793\n",
    "\n",
    "# For student 1 and 2, there is consensus to take Dr. Gerber, as the predicted ratings for Student_1 are very similar (4.388 vs 4.206) and for Student_2 are also very similar (4.252 vs 4.557). For Student_13, the predicted ratings are not similar (2.867 vs 4.359), so there is no consensus to take Dr. Gerber. One of these ratings is below the mean possible rating of 3, so there is no consensus to take Dr. Gerber for this student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3: 15 pts) Given the table from problem (1):\n",
    "# (a) By hand, apply item-item collaborative filtering with the cosine similarity score to predict the missing\n",
    "# ratings (on a scale of 1-10). Use the k = 2 most similar items to predict.\n",
    "# (b) Discuss in a few sentences how you would go about determining which algorithm (User-User or Item-Item)\n",
    "# is more accurate/produces better predictions.\n",
    "\n",
    "# Overall, I would determine the more accurate algorithm by finding the root mean squared error and the mean average error for both algorithms. \n",
    "# The algorithm with the lower RMSE and MAE would be the more accurate algorithm. \n",
    "# I would also consider the complexity of the algorithm when determining which algorithm is better. \n",
    "# While this circumstance only had 6 items and 4 users, a real-world dataset could contain thousands of items with thousands of users. \n",
    "# In this scenario, item-item collaborative filtering would be more computationally efficient than user-user collaborative filtering.\n",
    "# Meanwhile, user-user collaborative filtering would be more computationally efficient when the number of items is greater than the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4: 30 pts)\n",
    "# (a) In python update your collab filter() function from problem (2) by adding an additional argument type\n",
    "# which can be either \"User\" or \"Item\", where \"User\" runs the user-user collaborative filtering algorithm from (2),\n",
    "# while \"Item\" runs the item-item collaborative filtering algorithm. Recall that for item-item CF:\n",
    "# • We center the data based on the items (so you are calculating the mean of the Professors ratings now and\n",
    "# centering by column instead of by row).\n",
    "# • We are calculating pairwise similarity scores for all items/professors, while ignoring any users who have\n",
    "# not rated them. You might want to write a helper function that does this to avoid your collab filter()\n",
    "# function from getting too messy...\n",
    "# • When calculating the weighted similarity scores, if the target user has not rated one of the most similar items,\n",
    "# simply fill that missing value with the similar item’s mean.\n",
    "\n",
    "def item_similarity(data_centered, similarity_metric):\n",
    "    n_items = data_centered.shape[1]\n",
    "    sim_matrix = np.zeros((n_items, n_items))\n",
    "\n",
    "    for i in range(n_items):\n",
    "        for j in range(i + 1, n_items):\n",
    "            itemA = data_centered.iloc[:, i]\n",
    "            itemB = data_centered.iloc[:, j]\n",
    "            shared = ~np.isnan(itemA) & ~np.isnan(itemB)\n",
    "\n",
    "            if shared.sum() == 0:\n",
    "                sim = 0 \n",
    "            else:\n",
    "                if similarity_metric == \"Cosine\":\n",
    "                    sim = cosine_similarity(itemA[shared].values.reshape(1, -1),\n",
    "                                            itemB[shared].values.reshape(1, -1))[0, 0]\n",
    "                elif similarity_metric == \"L2\":\n",
    "                    sim = -np.linalg.norm(itemA[shared].values - itemB[shared].values)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid similarity metric. Requires 'Cosine' or 'L2'.\")\n",
    "\n",
    "            sim_matrix[i, j] = sim\n",
    "            sim_matrix[j, i] = sim\n",
    "\n",
    "    np.fill_diagonal(sim_matrix, 1)\n",
    "    return pd.DataFrame(sim_matrix, index=data_centered.columns, columns=data_centered.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collab_filter_user_item(data: pd.DataFrame, target_user: str, similarity_metric: str, k: int, type: str = \"User\"):\n",
    "    if type == \"User\":\n",
    "        return collab_filter(data, target_user, similarity_metric, k)\n",
    "    \n",
    "    elif type == \"Item\":\n",
    "        if target_user not in data.index:\n",
    "            return f\"Error: {target_user} not found in the dataset.\"\n",
    "\n",
    "        if data.loc[target_user].isnull().sum() == 0:\n",
    "            return f\"{target_user} has no missing ratings.\"\n",
    "\n",
    "        data_centered = data.sub(data.mean(axis=0), axis=1).fillna(0)\n",
    "\n",
    "        sim_df = item_similarity(data_centered, similarity_metric)\n",
    "        k = min(k, len(sim_df))\n",
    "\n",
    "\n",
    "        target_ratings = data.loc[target_user]\n",
    "        missing_items = target_ratings[target_ratings.isna()].index\n",
    "\n",
    "        preds = {}\n",
    "        for item in missing_items:\n",
    "            similar_items = sim_df[item].drop(index=item).dropna()\n",
    "            top_k_similar = similar_items.nlargest(min(k, len(similar_items)))\n",
    "\n",
    "            if top_k_similar.empty or top_k_similar.sum() == 0:\n",
    "                preds[item] = data.mean(axis=0)[item]\n",
    "            else:\n",
    "                filled_ratings = data[top_k_similar.index].loc[target_user].fillna(data.mean(axis=0)[top_k_similar.index])\n",
    "                preds[item] = np.dot(filled_ratings, top_k_similar) / top_k_similar.sum()\n",
    "\n",
    "        return pd.Series(preds)\n",
    "    \n",
    "    else:\n",
    "        return \"Error: Invalid type. Use 'User' or 'Item'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.402834432170603\n"
     ]
    }
   ],
   "source": [
    "# (b) Test that your function works for item-item CF by verifying that ”Student 5”, with k = 2 and using Cosine\n",
    "# similarity with type = ”Item”, has a predicted value of approximately 4.47 for Dr. Johnson.\n",
    "\n",
    "print(collab_filter_user_item(rmp_df, 'Student_5', 'Cosine', 2, 'Item')['Dr. Johnson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.388437826579327\n",
      "4.205697580213907\n",
      "4.251769420851983\n",
      "4.557496943759391\n",
      "2.867398438491762\n",
      "4.358856705459793 \n",
      "\n",
      "3.1213236504783155\n",
      "3.211987076168631\n",
      "3.6148643667643126\n",
      "3.823153706935148\n",
      "3.532088582183623\n",
      "3.593581983313002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (c) Repeat problem (2: c) but with item-item collaborative filtering and discuss if there are any differences.\n",
    "# CF Problem Ideation (15 points)\n",
    "# (c) Students 1, 2, and 13 are all considering taking Dr. Gerber (the first professor in the data) next Fall (they\n",
    "# have not taken him yet). Choose some value of k > 4. Use your function to predict what their ratings would be\n",
    "# under both the L2 and Cosine similarity metrics, and then decide if there is consensus for each student and\n",
    "# if so, would you recommend they take Dr. Gerber.\n",
    "\n",
    "print(collab_filter_user_item(rmp_df, 'Student_1', 'Cosine', 5, 'User')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_1', 'L2', 5, 'User')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_2', 'Cosine', 5, 'User')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_2', 'L2', 5, 'User')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_13', 'Cosine', 5, 'User')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_13', 'L2', 5, 'User')['Dr. Gerber'], \"\\n\")\n",
    "\n",
    "\n",
    "print(collab_filter_user_item(rmp_df, 'Student_1', 'Cosine', 5, 'Item')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_1', 'L2', 5, 'Item')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_2', 'Cosine', 5, 'Item')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_2', 'L2', 5, 'Item')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_13', 'Cosine', 5, 'Item')['Dr. Gerber'])\n",
    "print(collab_filter_user_item(rmp_df, 'Student_13', 'L2', 5, 'Item')['Dr. Gerber'])\n",
    "\n",
    "# 4.388437826579327\n",
    "# 4.205697580213907\n",
    "# 4.251769420851983\n",
    "# 4.557496943759391\n",
    "# 2.867398438491762\n",
    "# 4.358856705459793 \n",
    "\n",
    "# 3.1213236504783155\n",
    "# 3.211987076168631\n",
    "# 3.6148643667643126\n",
    "# 3.823153706935148\n",
    "# 3.532088582183623\n",
    "# 3.593581983313002\n",
    "\n",
    "# Overall, the predictions are lower when using Item-Item collaborative filtering compared to User-User collaborative filtering.\n",
    "# For all 3 students, the cosine similarity metric and L2 metric predicted recommending Dr. Gerber, as the predictions are all similar and greater than 3. \n",
    "# In each scenario, the L2 metric predicted a higher rating than the cosine similarity metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Discuss a real data problem and identify a corresponding data set, which you think would be appropriate to\n",
    "# analyze with Collaborative Filtering (or hybrid CF and Content-Based Filtering). You may think of using Collab-\n",
    "# orative Filtering (either user-user, or item-item) in either recommendation system or non-recommendation system\n",
    "# settings (you may want to discuss with the TAs or Dr. Gerber how that might work). The data set must, to\n",
    "# your knowledge, NOT have had Collaborative Filtering applied to it before (the TAs and Dr. Gerber will be\n",
    "# checking). Provide:\n",
    "# (a) A working link to the data set or to the website(s) from which the data set may be collected.\n",
    "# (b) A description of the problem of interest and how the data could be used to answer the problem with\n",
    "# Collaborative Filtering.\n",
    "# (c) Your intuition as to why Collaborative Filtering would be appropriate for this problem.\n",
    "\n",
    "# https://www.kaggle.com/datasets/ofurkancoban/discogs-datasets-january-2025/data\n",
    "\n",
    "# This dataset contains information about albums and artists on the Discogs platform. \n",
    "# A problem many individuals face is finding new artists to listen to while still being similar to their current preferences. With the use of the Discogs dataset, users could input their favorite artist (assuming they exist within the dataset) and receive recommendations for similar artists. The data includes descriptions of the artist, which could be used to create a content-based filtering system. \n",
    "\n",
    "# I have previously created a webapp project called MarinoBuddy that used cosine similarity to pair students with matching gym interests, and I think this could be similar. Because the artist description contains biographical information, such as the city and year of birth, it may be necessary to preprocess the data to try optimizing the search query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_files",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
